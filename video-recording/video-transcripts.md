Welcome to my presentation on JunctionTrees.jl: A Julia package for efficient Bayesian inference in probabilistic graphical models. Let me start by introducing a motivating example from the medical context, namely patient diagnosis, which will help me justify the need for our package. This is the sort of knowledge that a doctor might have about the relationship of different factors with a set of diseases: "shortness of breath, dyspnoea, may be due to tuberculosis, lung cancer or bronchitis, or none of them all, more than one of them. A recent visit to Asia increases the chances of tuberculosis, while smoking is known to be a risk factor for both lung cancer and bronchitis. The results of a single chest X-ray do not discriminate between lung cancer and tuberculosis, as neither does the presence or absence of this mean?" The main point here is that there is a lot of uncertainty involved in the relationship of the different factors in this example. And if the doctor wants to make a good diagnosis, he needs to take it into account. And this leads me to our proposed illusion, which is a Bayesian approach. A Bayesian approach is a principled way of reasoning under uncertainty by means of the fundamental laws of probability theory. In order to use a Bayesian approach, we first need to define a probabilistic model, which consists of a set of random variables and a joint probability distribution over these variables. In this example, we have eight binary random variables, which means that we have a state space of 256. Now imagine that we add not a binary random variable to our model. The number of states would duplicate and this is an issue since the state space of our model is growing exponentially with the addition of new random variables. This exponential growth makes it unfeasible to perform any kind of reasoning in a model that has a large number of variables, and therefore we need to tackle this. So the way to tackle this is to use the knowledge from the previous slide in order to make conditional independence assumptions about our model. The effect of these assumptions is a considerable reduction of the state space of our system. The way to incorporate these conditional independence assumptions is by defining a factorization for our joint distribution, which is what you see in that equation. This can also be represented graphically, for example, by means of a Bayesian network like the one you see here. The last step of our model specification is to define which subset of the variables we’re interested in and which subset of the variables we can observe, in other words, which subset of the variables we have values for. For our example, we will assume that we’re interested in the probability that the patient has tuberculosis, lung cancer, or bronchitis, these are the query variables, and that we know whether the patient has been to Asia or whether he suffers from dyspnoea. These are the observed variables. So let’s now summarize the problem statement and then proceed with our proposed solution. So given that a patient has dyspnoea and has recently visited Asia, calculate the probability that this person has either one of these diseases. But more generally, given a set of random variables and their joint distribution, compute one or more conditional distributions given observations, and we want to do this efficiently and for large and practical models. This leads us to JunctionTrees.jl, a Julia implementation of the Junction tree algorithm. But before I introduce JunctionTrees.jl, let me first give an overview of the Junction tree algorithm, which is an efficient method to perform Bayesian inference in probabilistic graphical models. By Bayesian inference, I mean computing conditional probabilities, like the ones we saw earlier. For an in-depth treatment of this topic, see that reference at the bottom of this slide. The first step of this algorithm is to transform the input graphical model, a Bayesian network in our case, into a secondary structure known as a junction tree by means of graphical transformations. The first of these transformations is known as moralization, and it involves marrying (connecting) the parts of each variable and dropping the direction of the edges. Their result is a moral graph. The next transformation, and probably the most difficult and important one, is known as triangulation. It consists of removing cycles of length greater than three, like this one, for example. And we do so by connecting non-adjacent nodes like this two for example. The goal is to do this in a way that minimizes the resulting state space of the secondary structure. This is known to be NP-hard. The result is a triangulated graph. A clique is a subset of variables that are all connected with each other. A maximal clique is a clique that is not contained inside the larger clique. These are the cliques of the triangulated graph, which become the nodes or the clusters of our junction tree. Note the set of variables that connect each cluster, like this one, for example. These are known as separation sets or sepsets. They are the intersection of the variables of the adjacent clusters. The most important characteristic of the junction tree is known as the running intersection property. It says that if two clusters contain the same variable, then the clusters in the path connecting those two clusters, must contain that variable too. The next step is to initialize the junction tree. This corresponds to assigning the conditional distributions that were given as part of the model definition into a cluster that covers its variables. So for example, if we have E given T and L, we can allocate that conditional distribution into this cluster because it covers its variables. Now we need to integrate the observations into our model. This corresponds to invalidating any state that is not in accordance with the values observed. At this point, we have an inconsistent junction tree, so the next step is to propagate the evidence that is locally stored in each cluster to all other clusters in the graph. This is done in two stages: an inward and an outward path. First, we need to randomly select a root. Then we need to propagate messages from the leaves towards the root. And then from the root towards the leaves. This is a valid schedule. Now that we have a consistent junction tree, we need to find the variables of interest via marginalization. The first step is to locate a subset or a cluster that contains the firewall of interest. For example, if we want to extract the marginal of B, then we can do it from this sepset. To do so, we need to multiply the two incoming messages to this sepset, and then sum out the other variable, which in this case is L. And the final step is to normalize the resulting conditional distributions. This means that we have to make sure that the sum of the probabilities of each possible value of the random variable equals to one. The end result are the conditional distributions over the variables of interest given the evidence. These are also known as marginals or posterior distributions. And finally, we arrive to JunctionTrees.jl, our Julia implementation of the junction tree algorithm. I want to give a quick shout-out to Graphs.jl and the MetaGraphs.jl, which are two packages that our package heavily relies on. Just like we saw earlier with the Asian network example, this package allows you to compute the posterior marginal distributions of discrete variables given evidence, perhaps more interestingly, and what distinguishes this package from other implementations in other programming languages, is the fact that we use Julia’s metaprograming capabilities to separate the algorithm in two stages: a compilation on our runtime phase. This allows us to pre-compute a lot of computations in the compilation phase in order to generate a light and faster program at runtime. Finally, we offer functionality to visualize the junction trees, Bayesian networks, and Markov random fields using Makie.jl, specifically GraphMakie.jl. Here's a runtime performance comparison between Merlin, a C++ library that implements the junction tree algorithm and JunctionTrees.jl. We use the Promedas data set, which comes from the medical domain, and it consists of 28 different problems of varying complexity. We show better performance in each of these problems. As mentioned earlier, our implementation uses metaprogramming to separate those computations that can be done at compile time with those that need to be done at runtime. Specifically, the construction of the junction tree is something that can be done at compile time. Here’s the architecture of the metaprogramming-based framework. You feed in the query variables, the Probabilistic Graphical Model, and the evidence variables to the junction tree algorithm. This outputs a subject program in the form of a Julia expression. This Julia expression can then be fed into a series of optimization passes which result in a specialized program that can then be evaluated online with the online evidence values in order to compute the posterior marginal probabilities. We already experimented with this idea and as a result, we implemented a partial evaluation optimization pass, which basically analyzes all the message dependencies at compile-time, and if they do not depend on online values, then they can be pre-computed in the compilation stage. The messages in green in this graph are those that do not depend on online messages. And these are the results after applying the optimization. So in the left you have before the optimization and the bar on the right is after the optimization for each of the problems. And we managed to decrease the average performance by 21%. This idea will be published in a paper in DSD 2022. And with this, I would like to conclude my presentation. Thank you for your time and I’ll be happy to answer some questions.
